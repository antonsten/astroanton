---
title: Designer Ethics & The Moral Implications of our Apps
description: A short reflection on user experience and design.
date: 2018-06-11
readingTime: 8
---

Lawyers, doctors, and even journalists have something in common. They all have studied ethics as part of their higher education. They’ve taken the time to construct, interpret, and follow written code of conducts that guide us in making good, ethical decisions. Is it a coincidence that these are also some of the world’s oldest professions?

The design profession has a long history too, but it bothers me that there are hardly any discussions on ethics surrounding it. Design is all about viewer; drawing the eye and changing the heart. It’s truly one of the most powerful tools (superpowers?) that companies have today and that leaves us - the viewers - wanting… no, needing awareness and guidelines that ensure design is being used responsibly. With all the services and apps influencing us, is this too much to ask?

## What are ethics?

Ethics can be a tricky subject to approach. Simply said, ethics are an agreed up set moral principles (rules) within a profession/community leading to consistency in behavior and conduct. Looking at those older professions we talked about earlier shows how complicated it can be, but how necessary it is.

**Lawyers** are always required to work in the best interest of their client. Not themselves or what they personally think is “right”. It’s a keystone of our society - the right to legal representation and a fair trial.

**Journalists** — who we might think are just out to sell magazines (e.g get clicks) — are guided by a code of conduct as well. People aren’t to be judged before they’re convicted and details are to be spared when there’s children involved for instance.

**Medical doctors** have an ethical duty to protect the human rights and dignity of their patients - some of the most vulnerable members of our society. Doctors are guided by a common framework of four principles:

- Respect for autonomy – the patient has the right to refuse or choose their treatment.
- Beneficence – a practitioner should act in the best interest of the patient.
- Non-maleficence – to not be the cause of harm. Also, "utility" - to promote more good than harm.
- Justice – concerns the distribution of scarce health resources, and the decision of who gets what treatment (fairness and equality).

On top of those, they are required to have:
- Respect for persons – the patient, family, and practitioner (may or may not be a doctor) have the right to be treated with dignity.
- Truthfulness and honesty – the concept of informed consent has increased in importance since the historical events of the Nuremberg Trials and Tuskegee Syphilis Experiment where physicians deceived patients.


## The time of “move fast and break things” is over.

I can’t begin to understand why there’s no common framework for designers. Everyone can agree that the products we create have a major impact on society and our individual social identities, but there is very little to no open discussion on the implications of those products.

Google’s <a href="/ai-ethics/">recent demonstration of Duplex</a> and the <a href="https://twitter.com/BridgetCarey/status/993910061209702400">discussions that followed</a> are surely a first step, but one has to wonder how that feature even got featured on the main stage? It’s clearly a problem if Google - one of the world’s largest companies and drivers of technical advancement - are able to design and build a feature like that without raising any critical questions internally?

Don’t get me wrong, I’m all for indicatives like the <a href="http://humanetech.com">Time Well Spent initiative</a> and Apple’s newly launched <a href="https://www.apple.com/newsroom/2018/06/ios-12-introduces-new-features-to-reduce-interruptions-and-manage-screen-time/">Screen Time</a>. My concern is that we can’t just keep building new tools to bandaid what we shouldn’t have broken in the first place. We use our devices too much, yes, but is another app really the solution for it? I believe it’s time for our industry to stop trying to fix the problems and start acting more pro-actively at not creating problems in the first place. The time of “move fast and break things” is over.

Facebook doesn’t need regulation; it may help, but regulation alone won’t fix the problems. Facebook needs moral guidance. In both hearings on Capital Hill, Mark Zuckerberg reiterated that this was not what he had in mind when he created Facebook in his dorm room. I’m sure it wasn’t, but here we’re treating Facebook as if it’s something people in dorm rooms all over the world would still like to create. Our industry needs to learn a lesson. Mark may have created the Facebook platform, but he didn’t create ALL the problems. We, the designers, did the rest. Someone had to create the third party apps that stole our private data. We were failed by those who valued themselves over their users.

>“The superior man understands what is right; the inferior man understands what will sell.”**― Confucius**


## A code of conduct for designers?

Instead of focusing on holding ourselves to higher ethical standards, we just come up with other names for bad behavior. We give it a cool name like ‘dark patterns’ and accompany it with a picture of Darth Vader in-front of a computer. Awesome, right? How about we just call <a href="https://darkpatterns.org/">‘dark patterns’</a> what they truly are - bad design and bad ethics. It’s dishonest and it’s short sighted.

While the GDPR may not be perfect (and again, I don’t think legislation alone will solve this), but I think it’s an important first step. Let’s take a look back at the ethical duties of doctors and try to make it fit designers. It’s surprising how easily the principles can be adapted:
- Respect for autonomy – the user has the right to refuse or choose what content they are presented with. No one has the authority to bypass this without penalty. This is what GDPR aims to do.
- Beneficence – the app/service should act in the best interest of the user. No more dark patterns. No more use of user data without consent.
- Non-maleficence – to not be the cause of harm. Also, “utility" - to promote more good than harm. Essentially Google’s original ‘Don’t do evil’ promise. No designer can intentionally cause harm or diminish the value of the user. Google is currently under some heavy fire with employees leaving because of it’s <a href="https://www.engadget.com/2018/03/06/google-is-helping-us-military-train-ai-to-study-drone-footage/">affairs with the US Military</a>.
- Justice – concerns the distribution of resources, and the decision of who gets what service (fairness and equality). All users are equal and all content providers are equal. Balanced access for all (read: net-neutrality).

We’re just seeing the beginning of AI concerns and understanding the need for <a href="/ai-ethics/">ethics in the industry</a>. As voice assisted user interfaces bring up entirely new scenarios, we need to think about what kind of technology we want in our lives. We need to have the choice to opt out of those Google Duplex calls and to Alexa and Google home listening when they shouldn’t. If we all, as designers, agree to ethical standards, situations like this wouldn’t even be an issue. We’d all ask ourselves, “just because we can, does that mean we should?”

While it’s still fiction, the dilemmas of a <a href="https://www.hbo.com/westworld">Westworld</a> similar experience is not as far away as we’d like to think. If you’re not familiar with Westworld, I highly suggest you have a look. Briefly put, Westworld is a series about an AI-powered reality housed in a big theme park set in the Wild West where visitors are allowed to live out their wildest dreams. The habitants of this world are all robots that look, act, and respond in a very lifelike way - just like the Google Assistant in the Duplex call. What’s interesting about the show though is the philosophical questions - what really distinguishes us from the robots?

<blockquote class="twitter-tweet" data-cards="hidden" data-lang="en"><p lang="en" dir="ltr">Today we’re sharing our AI principles and practices. How AI is developed and used will have a significant impact on society for many years to come. We feel a deep responsibility to get this right. <a href="https://t.co/TCatoYHN2m">https://t.co/TCatoYHN2m</a></p>&mdash; Sundar Pichai (@sundarpichai) <a href="https://twitter.com/sundarpichai/status/1004800469405876226?ref_src=twsrc%5Etfw">June 7, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

**Update:** And just like that, <a href="https://twitter.com/sundarpichai/status/1004800469405876226">Google has released a statement</a> that pretty closely aligns with everything we talked about here. It’s true that there’s a big difference between making a statement and applying it to day-to-day business, but I see this as a good sign. Hopefully other designers of AI platforms take a cue from this statement and take responsibility.

Uncle Ben from Spiderman said it best, _“With great power comes great responsibility.”_