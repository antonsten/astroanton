---
slug: ethics-in-design-part-1
title: "Ethics in Design - Part 1"
description: A short reflection on user experience and design.
date: 2021-05-31
readingTime: 17
---

*I’m currently leading the “Design Ethics” class for Hyper Island’s UX Upskill Program. This is part 1 out of 2. <a href="/design-ethics-part2">Part 2 is here.</a>*

Approaching ethics is tricky by default. It should be simple as ethics are basically an agreed up set moral principles leading to consistent behaviour and conduct. However, our interpretation of these rules is up to us as individuals and what we feel is morally acceptable.

Believe it or not, lawyers, doctors, and even journalists have something in common. They all have had to study ethics as part of their higher education. They’ve taken the time to construct, interpret, and follow a written code of conduct that guides them in making good, ethical decisions. Is it a coincidence that these are also some of the world’s oldest, most respected professions?

**Lawyers** are always required to work in the best interest of their client often disregarding themselves or what they personally think is “right”. It’s a keystone of our society - the right to legal representation and a fair trial.

**Journalists** are those who we might think are just out to sell magazines (e.g get clicks) at first glance, but they are guided by a code of conduct as well. A few examples of this are that people aren’t to be judged before they’re convicted and details are to be spared when there’s children involved.

**Medical doctors** have an ethical duty to protect the human rights and dignity of their patients - some of the most vulnerable members of our society. Doctors are guided by a common framework of four principles:

- Respect for autonomy – the patient has the right to refuse or choose their treatment.
- Beneficence – a practitioner should act in the best interest of the patient.
- Non-maleficence – to not be the cause of harm. Also, “utility” - to promote more good than harm.
- Justice – concerns the distribution of scarce health resources and the decision of who gets what treatment (fairness and equality).

On top of those, they are required to have:

- Respect for persons – the patient, family, and practitioner (may or may not be a doctor) have the right to be treated with dignity.
- Truthfulness and honesty – the concept of informed consent has become crucial since the historical events of the Nuremberg Trials and Tuskegee Syphilis Experiment were examples of physicians deceiving patients.

## So where does that leave us as designers?

While we may not guide people between life and death as both doctors and lawyers do, we do play a serious role in the future and fate of not just individual humans, but more so humanity. We’re not just responsible for what we choose to design, but just as much, what we choose *not* to design. Those of you who remember the Internet 20 years ago can probably agree that while the Internet was far from perfect then, it wasn’t filled with as much hate, anger, and prejudice as today.

>Social media has been described as more addictive than cigarettes and alcohol, and is now so entrenched in the lives of young people that it is not longer possible to ignore it when talking about young people’s mental health.**<a href="https://twitter.com/shirleycramer28?lang=en">Shirley Cramer, chief executive, Royal Society for Public Health</a>**

Social media has made anxiety and depression skyrocket among teenagers (also resulting in sleep disturbances). The fake news phenomenon has affected local and global politics and elections. What will the next 20 years look like?  That’s up to you.

> You are responsible for what you put into the world. You are defined by the clients you take on, and you can only stand as proud of the work as its benefit to society entitles you to. I urge each and every one of you to seek out projects that leave the world a better place than you found it. We used to design ways to get to the moon; now we design ways to never have to get out of bed. You have the power to change that.**<a href="https://twitter.com/monteiro">Mike Monteiro</a>**

Sure. I get it. Getting a job at Facebook might be your big break. Hey, I know. Getting *a job* isn’t to be taken lightly. We've all got bills to pay and mouths to feed. The thing we need to ask ourselves is where - and when - do we ultimately draw the line? Gordon Gekko said, "Greed is good" but I'm not so sure, especially not when greed ultimately controls our elections, affects our thinking and our mental health.

> We are paid a ton. Looking forward to my yearly bonus of $100K. <br />Fuck ethics. Money is everything.
**<a href="https://www.teamblind.com/company/Facebook/">Anonymous Facebook employee posting on Blind, an employee gossip site.</a>**

It’s going to take a lot of courage to head right in there and start demanding change, but here’s the tough truth, change is needed. Do you know who is responsible for making that change happen? You. Me. Not them. Not someone else. It’s you and me. I want to believe that most of you still know where that line needs to be drawn because once we pass it, it's incredibly difficult to go back. Finding the balance to support business goals (to keep those churn rates down) clearly shows us the unfriendly user behavior is what needs to addressed.

> “You know that line about every group of friends having at least one asshole and if you can’t figure out who it is, it’s probably you? Right. Well, every place you work has someone responsible for making sure the work you’re doing is ethically sound. I’ll give you a minute to figure out who it is.<br /><br />
Please tell me you got that.”**<a href="https://twitter.com/monteiro">Mike Monteiro</a>**

## What’s your moral ground?

The water gets muddy way too often. As I said before, we like to believe that we all have a similar moral compass, but that simply isn't true. It seems that whatever brings in money, regardless of consequences, can be justified. Honestly, this is why weapons of mass destruction exist. It isn't so much if we can do something, it's if we should or not. Some even believe it isn't morally acceptable to create website as it will consume power which may, or may not, come from coal mining.

## So let’s gut check our own moral compasses

20 years ago, there were primarily three main industries that were to be morally questionable as they fed people's vices:

- Alcohol (but working for Absolut Vodka in the 1990’s and early 2000's was seen as really cool)
- Tobacco (yet way more people were smoking back then)
- Gambling (and this was pre online casinos)

Still, you easily understand the morals and ethics as each led to addictions that we had experienced throughout centuries.

As the Internet scaled, the list of questionable industries grew:

- Porn (still one of the biggest industries online)
- Gig-economy (underpaid workers without any safety net at all)
- Social media (well, you know...)

And more recently, what about:

- Bitcoin (consuming more power than the entire country of Argentina or Holland)
- Voice assistants (<a href="https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio">quietly listening in on conversations at home</a> removing whatever little privacy you still had)

Let's make it even muddier:

Uninformed, ethically-devoid design choices are increasingly impacting every product and service we use. The less attention to diversity we have, the more likely we’re going to design something that is only relevant for those just like ourselves. When even a simple <a href="https://reporter.rit.edu/tech/bigotry-encoded-racial-bias-technology">thing like a soap dispenser can be racist,</a> it’s easy to understand how features need to account for diversity.

> The dispenser used near-infrared technology to detect hand motions, an article on Mic read. The invisible light is reflected back from the skin which triggers the sensor. Darker skin tones absorb more light, thus enough light isn't reflected back to the sensor to activate the soap dispenser. Which means that dark-skinned restroom users will have to skip washing their hands with this not-so-sensitive soap dispenser.
<br /><br />
This epic design flaw may seem hilarious on the internet, but demonstrates a major issue with many technology-based companies: diversity. The soap dispenser was created by a company called Technical Concepts, which unintentionally made a discriminatory soap dispenser because no one at the company thought to test their product on dark skin.
<br /><br />
“If you have an office full of white people, whatever products that come out of that office are more likely to be geared more towards white people. The less diversity there is in a workplace environment, the more likely major design flaws will be present that only affect people of color,”**<a href="https://reporter.rit.edu/tech/bigotry-encoded-racial-bias-technology">BIGOTRY ENCODED: RACIAL BIAS IN TECHNOLOGY</a>**

So where’s your moral ground? What do you find acceptable? Where is the line drawn?

## User privacy

> Arguing that you don't care about the right to privacy because you have nothing to hide is no different than saying you don't care about free speech because you have nothing to say.
**<a href="https://twitter.com/Snowden">Edward Snowden</a>**

Luckily, over the last few years there's been more and more attention given to user's privacy. Laws like the GDPR, which ultimately puts users in control of their own data (or at least that's what it's meant to), are steps in the right direction.

In 2016, Facebook exposed the data of 87 million users to Cambridge Analytica, a company working for the Trump campaign. In 2018, Mark Zuckerberg was pulled before congress for questioning. Democratic congresswoman Anna Eshoo asked him: “Are you willing to change your business model to protect users’ privacy?” His reply? “Congresswoman, I don’t know what that means.”

It's easy to imagine that this would only concern Facebook users though, right? I've even heard numerous friends argue that while they do have a Facebook account, they're not active so what harm does it do? It doesn't matter if you are active or not, Facebook still collects data about you. In fact, it doesn't even matter if you have an account or not - they collect data about you. The data they collect for ad-purposes on their users (and even non-users, aka shadow profiles).

> The most powerful example came from Rep. Ben Luján (D-NM), who confronted Zuckerberg on the company’s use of shadow profiles — a term for non-user data collection that Zuckerberg was apparently unfamiliar with.
<br /><br />
“It’s been admitted that you do collect data points on non-Facebook users,” Luján asked. “So my question is, can someone who does not have a Facebook account opt out of Facebook’s involuntary data collection?”
“Congressman, anyone can opt out of any data collection for ads, whether they use our services or not,” Zuckerberg said. “But in order to prevent people from scraping public information, we need to know when someone is trying to repeatedly access our services.”
<br /><br />
“You’ve said everyone controls their data, but you’re collecting data on people who are not even Facebook users, who never signed a consent or privacy agreement and you’re collecting their data,” Luján continued. “And you’re directing people who don’t have a Facebook page to sign up for Facebook in order to get their data.”**<a href="https://www.theverge.com/2018/4/11/17225482/facebook-shadow-profiles-zuckerberg-congress-data-privacy">Shadow profiles are the biggest flaw in Facebook’s privacy defense</a>**

Facebook intentionally designed a system that can collect data without user consent and in order to delete that data, you have to sign up for a Facebook account. This isn't by chance, this is designed. And unfortunately, it was created by designers like you and me.

> While the products we build every day have evolved, our ways of thinking about user engagement have scarcely changed since the days of dial-up. We assume that the higher the metrics, the more successful the business and, thus, the smarter the product designer.
**<a href="https://essays.uxdesign.cc/tech-diet/">The world needs a tech diet</a>**

We claim to design for the users but the metrics we go by are all driven by the business.

- Clicks
- Views
- Session duration
- Shares
- Sign-ups
- Avg time on page
- Pages per visit

If we'd truly considered the user first, we'd consider as low numbers and metrics as possible as win, right? It may feel counter-intuitive, but less clicks, shorter session durations, less time spent on a page, and fewer pages per visit are actually a win. The quicker we can solve the user's request, the better. But most business' goal is to lure them in and make them stay.

In fact, we've even come to the point where we think of success not by what services are getting traction, but by services deprive us of something *we know* is crucial to our well-being; sleep. The worst part? We consider it an even greater win and a new metric 'to beat'!

> “You get a show or a movie you’re really dying to watch, and you end up staying up late at night, so we actually compete with sleep,” he said of his No. 1 competitor. Not that he puts too much stock in his rival: “And we’re winning!”**<a href="https://www.fastcompany.com/40491939/netflix-ceo-reed-hastings-sleep-is-our-competition">Netflix CEO Reed Hastings: Sleep Is Our Competition</a>**

Next week, we'll start to look at the patterns designers use to make products less user-friendly and more business-optimised. I want to make it absolutely clear that I don't consider the two to be opposites, but I do think it's 100% possible to create a service that is revenue-generating *and* user-friendly without forgoing optimization. But it does require difficult conversations and pinpoint focus.

> “People think focus means saying yes to the thing you've got to focus on. But that's not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully. I'm actually as proud of the things we haven't done as the things I have done. Innovation is saying no to 1,000 things.”**<a href="https://www.apple.com">Steve Jobs</a>**

## What's human? And what's humane?

For the last years, AI sure has been the talk of the town. From how it will empower our digital experiences to how it will revolutionise science. And, of course on the other end of the spectrum, how it'll make all of us unemployed and everything will ultimately be run by AI algorithms and computers.

AI has our attention.

The possibilities continually clog our news feeds, create interesting conversations, and give tech leaders inspiration to explore unique solutions. What will the development of this technology look like? What will this mean for us as humans? How will this impact society? With all the questions being asked, only one thing is absolutely clear. We’re about to enter one of the biggest transformations our society has witnessed in the last century - if not millennium.

<a href="https://soundcloud.com/alexismadrigal/google-duplex-calling-a-restaurant?utm_source=www.antonsten.com&utm_campaign=wtshare&utm_medium=widget&utm_content=https%253A%252F%252Fsoundcloud.com%252Falexismadrigal%252Fgoogle-duplex-calling-a-restaurant">Google Duplex scheduling a hair salon appointment by AlexisMadrigal</a>

Anything you find odd in the clip? No? What if I told you that the 'person' making the appointment was Google's Assistant. As if defining ethics between humans was difficult enough, we're having to define ethics between humans and machines. And to add to that complexity, we're becoming increasingly unaware of what a machine is.

<a href="https://open.spotify.com/album/2K1s5O6IOpYDAG7hV1X5lf?utm_source=embed_v2&go=1&play=1&nd=1">Classical Music Composed by Computer: Experiments in Musical Intelligence</a>

David Cope has written programs that compose concertos, chorales, symphonies and operas. His first creation was named EMI (Experiments in Musical Intelligence), which specialised in imitating the style of Johann Sebastian Bach. It took seven years to create the program, but once the work was done, EMI composed 5,000 chorales à la Bach in a single day.

Cope arranged a performance of a few select chorales in a music festival at Santa Cruz. Enthusiastic members of the audience praised the wonderful performance, and explained excitedly how the music touched their innermost being. They didn’t know it was composed by EMI rather than Bach, and when the truth was revealed, some reacted with stunned silence while others shouted in anger.

> I am genuinely bothered and disturbed at how morally wrong it is for the Google Assistant voice to act like a human and deceive other humans on the other line of a phone call, using upspeak and other quirks of language. “Hi um, do you have anything available on uh May 3?”
If Google created a way for a machine to sound so much like a human that now we can’t tell what is real and what is fake, we need to have a talk about ethics and when it’s right for a human to know when they are speaking to a robot.
In this age of disinformation, where people don’t know what’s fake news… how do you know what to believe if you can’t even trust your ears with now Google Assistant calling businesses and posing as a human? That means any dialogue can be spoofed by a machine and you can’t tell.**<a href="https://twitter.com/BridgetCarey/status/993910061209702400">Bridget Carey, CNN</a>**

I think we’ve reached the point now where the differences between companies like Apple and Google are becoming much more obvious. For years, the consensus has been that Apple has lagged behind Google in AI/personal assistants and we’re starting to understand why that is. I've always believed that Apple is a product-driven company focused on human needs whereas Google is a technology-driven company focused on leveraging data. It’s a fundamentally different way of valuing the user - the human.

The real danger with a service like Google Assistant voice? If you ask Oxford University or Ball State University one of the most likely jobs to be automated in the near future is telemarketers. Telemarketing is already a problem today and we still have humans making the calls - just imagine what will happen once you extract the cost of humans from telemarketings companies. Your phone may become as filled with spam as your email is.

> Technology isn’t an industry, it’s a method of transforming the culture and economics of existing systems and institutions. That can be a little bit hard to understand if we only judge tech as a set of consumer products that we purchase. But tech goes a lot deeper than the phones in our hands, and we must understand some fundamental shifts in society if we’re going to make good decisions about the way tech companies shape our lives—and especially if we want to influence the people who actually make technology.**<a href="https://medium.com/humane-tech/12-things-everyone-should-understand-about-tech-d158f5a26411">12 Things everyone should know about tech</a>**

Yuval Noah Harari, author of Homo Sapies and Homo Deus, argues that liberalism will eventually fade away as we eventually trust the algorithm more than we trust ourselves. Ray Dalio, successful investor has already switched his company to radical transparency using a point system that analyzes data to rate people’s ‘believability’ rather than operating through democracy or even hierarchy. I highly recommend you to watch his <a href="https://www.ted.com/talks/ray_dalio_how_to_build_a_company_where_the_best_ideas_win">TED Talk: How to build a company where the best ideas win</a> as an example of how AI can make us more honest, transparent, and guide better decision making.

Simply put, there’s a rapidly growing urgency for us to have serious conversations about our ethical responsibilities. This goes  for the products we create as well as the products we choose to use. Everything is still so new. There’s been no real direction or consensus to help us determine what we should consider OK and what is way out of line. Our world is moving so quickly and we hardly ever stop and consider the ethics of our choices. Instead, we simply see a machine calling a hairdresser, think how awesome that is, and move on with our day…

*I’m currently leading the “Design Ethics” class for Hyper Island’s UX Upskill Program. This is part 1 out of 2. <a href="/design-ethics-part2">Part 2 is here.</a>*